{"paragraphs":[{"text":"%md\n### Bienvenue dans ce quatrième exercice\n\nL'objectif va être de vous initier aux Datasets en scala","dateUpdated":"2020-10-05T10:59:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Bienvenue dans ce quatrième exercice</h3>\n<p>L&rsquo;objectif va être de vous initier aux Datasets en scala</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1601895541286_-1397825004","id":"20180226-214315_1711172483","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:41169"},{"text":"%spark\n\n// Q1 - Chargez dans un dataframe le fichier json situé dans \"/data/data/people/people.json\".\nimport org.apache.spark.sql.types._\n\n\nval jsonData = spark.read.json(\"/data/data/people/people.json\")","dateUpdated":"2020-10-05T10:59:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types._\njsonData: org.apache.spark.sql.DataFrame = [age: bigint, first_name: string ... 4 more fields]\n"}]},"apps":[],"jobName":"paragraph_1601895541286_-1397825004","id":"20180226-214415_1768818666","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41170"},{"text":"%spark\n\n// Q2 - Une bonne pratique est de toujours visualiser ses données. Affichez votre DataFrame avec la méthode \"show\".\n\njsonData.show","dateUpdated":"2020-10-05T10:59:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+----------+------+---+-------------+--------------------+\n|age|first_name|gender| id|    last_name|                text|\n+---+----------+------+---+-------------+--------------------+\n| 24|      Brok|  Male|  1|    Springate|vel accumsan tell...|\n| 99|     Trish|Female|  2|      Fielden|at feugiat non pr...|\n| 77|   Humfrey|  Male|  3|      Sparrow|mollis molestie l...|\n| 77|    Merrie|Female|  4|      Roddick|odio odio element...|\n| 51|  Isabella|Female|  5|  Myderscough|convallis tortor ...|\n| 30|    Blisse|Female|  6|        Caron|sit amet nulla qu...|\n|  8|  Gamaliel|  Male|  7|    Coulthart|at nibh in hac ha...|\n| 39|   Candide|Female|  8|   Tollemache|orci luctus et ul...|\n| 89|   Renaldo|  Male|  9|     Christin|eget elit sodales...|\n|  7| Stanfield|  Male| 10|       Pollak|eget eleifend luc...|\n| 93|    Leonid|  Male| 11|      Riddoch|nisi vulputate no...|\n| 76|     Chris|Female| 12|         Tape|vitae quam suspen...|\n| 90|    Sawyer|  Male| 13|      Bagnold|suscipit a feugia...|\n| 83|  Reinwald|  Male| 14|      Parslow|vestibulum alique...|\n| 71| Westbrook|  Male| 15|     Crewther|nunc proin at tur...|\n| 42|     Viole|Female| 16|      Blincko|curae mauris vive...|\n| 26|      Eddi|Female| 17|     Goligher|sit amet eros sus...|\n| 57|   Siobhan|Female| 18|     Leathart|tempus vivamus in...|\n| 24|  Northrop|  Male| 19|      Barkway|volutpat convalli...|\n|  9|   Myrtice|Female| 20|Grisenthwaite|hendrerit at vulp...|\n+---+----------+------+---+-------------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1601895541286_-1397825004","id":"20180227-182421_1059045001","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41171"},{"text":"%md\n\nPour l'instant nous venons de créer un DataFrame, donc un Dataset[Row].\nNous pouvons le vérifier, Zeppelin affiche dans la zone de résultat le type des variables qui sont crées.","dateUpdated":"2020-10-05T10:59:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Pour l&rsquo;instant nous venons de créer un DataFrame, donc un Dataset[Row].<br/>Nous pouvons le vérifier, Zeppelin affiche dans la zone de résultat le type des variables qui sont crées.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1601895541286_-1397825004","id":"20180226-215443_138650667","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41172"},{"text":"%spark\n\n// En scala sela donne :\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.sql.Row\n\nval jsonDataset: Dataset[Row] = jsonData\n","dateUpdated":"2020-10-05T10:59:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.Dataset\nimport org.apache.spark.sql.Row\njsonDataset: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [age: bigint, first_name: string ... 4 more fields]\n"}]},"apps":[],"jobName":"paragraph_1601895541286_-1397825004","id":"20180226-215127_1166653665","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41173"},{"text":"%md\n\nNous allons maintenant spécifier un encoder pour ne plus avoir un Dataset[Row] mais un Dataset[Person]\n\nLa première étape est de créer une classe Scala\n\nUne classe scala peut se définir de la façon suivante : \n```case class Point(x: Int, y: Int)```","dateUpdated":"2020-10-05T10:59:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Nous allons maintenant spécifier un encoder pour ne plus avoir un Dataset[Row] mais un Dataset[Person]</p>\n<p>La première étape est de créer une classe Scala</p>\n<p>Une classe scala peut se définir de la façon suivante :<br/><code>case class Point(x: Int, y: Int)</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1601895541287_-1398209753","id":"20180226-215947_830340587","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41174"},{"text":"%spark\n\n// Q3 - Créez la classe Person\ncase class Person(id:Long, first_name: String, last_name:String, gender:String, age:Long, text: String )\n","dateUpdated":"2020-10-05T10:59:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Person\n"}]},"apps":[],"jobName":"paragraph_1601895541287_-1398209753","id":"20180226-220130_1059325388","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41175"},{"text":"%md\n\nTransformer un Dataframe en un Dataset se fait de la manière suivante:\n```val ds = spark.read.json(\"/datasource.json\").as[myClass]```","dateUpdated":"2020-10-05T10:59:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Transformer un Dataframe en un Dataset se fait de la manière suivante:<br/><code>val ds = spark.read.json(&quot;/datasource.json&quot;).as[myClass]</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1601895541287_-1398209753","id":"20180226-220531_1238174906","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41176"},{"text":"%spark\n\n// Q4 - Transformez votre Dataframe en un Dataset[Person].\n\nval ds = jsonData.as[Person]","dateUpdated":"2020-10-05T10:59:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"ds: org.apache.spark.sql.Dataset[Person] = [age: bigint, first_name: string ... 4 more fields]\n"}]},"apps":[],"jobName":"paragraph_1601895541287_-1398209753","id":"20180226-220623_528898502","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41177"},{"text":"%md\n\nVous devez bien obtenir un Dataset[Person]\n\nFaites un .show() à la fois sur votre DataFrame et sur votre nouveau Dataset.\n","dateUpdated":"2020-10-05T10:59:01+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Vous devez bien obtenir un Dataset[Person]</p>\n<p>Faites un .show() à la fois sur votre DataFrame et sur votre nouveau Dataset.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1601895541287_-1398209753","id":"20180226-215809_447424511","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41178"},{"text":"%spark\n\n// Q5 - Affichez votre DataFrame de départ et sur votre nouveau Dataset. L'affichage doit être identique\n\njsonData.show","dateUpdated":"2020-10-05T10:59:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+----------+------+---+-------------+--------------------+\n|age|first_name|gender| id|    last_name|                text|\n+---+----------+------+---+-------------+--------------------+\n| 24|      Brok|  Male|  1|    Springate|vel accumsan tell...|\n| 99|     Trish|Female|  2|      Fielden|at feugiat non pr...|\n| 77|   Humfrey|  Male|  3|      Sparrow|mollis molestie l...|\n| 77|    Merrie|Female|  4|      Roddick|odio odio element...|\n| 51|  Isabella|Female|  5|  Myderscough|convallis tortor ...|\n| 30|    Blisse|Female|  6|        Caron|sit amet nulla qu...|\n|  8|  Gamaliel|  Male|  7|    Coulthart|at nibh in hac ha...|\n| 39|   Candide|Female|  8|   Tollemache|orci luctus et ul...|\n| 89|   Renaldo|  Male|  9|     Christin|eget elit sodales...|\n|  7| Stanfield|  Male| 10|       Pollak|eget eleifend luc...|\n| 93|    Leonid|  Male| 11|      Riddoch|nisi vulputate no...|\n| 76|     Chris|Female| 12|         Tape|vitae quam suspen...|\n| 90|    Sawyer|  Male| 13|      Bagnold|suscipit a feugia...|\n| 83|  Reinwald|  Male| 14|      Parslow|vestibulum alique...|\n| 71| Westbrook|  Male| 15|     Crewther|nunc proin at tur...|\n| 42|     Viole|Female| 16|      Blincko|curae mauris vive...|\n| 26|      Eddi|Female| 17|     Goligher|sit amet eros sus...|\n| 57|   Siobhan|Female| 18|     Leathart|tempus vivamus in...|\n| 24|  Northrop|  Male| 19|      Barkway|volutpat convalli...|\n|  9|   Myrtice|Female| 20|Grisenthwaite|hendrerit at vulp...|\n+---+----------+------+---+-------------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1601895541287_-1398209753","id":"20180226-215305_1356420806","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41179"},{"text":"%spark\n\nds.show","dateUpdated":"2020-10-05T10:59:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+----------+------+---+-------------+--------------------+\n|age|first_name|gender| id|    last_name|                text|\n+---+----------+------+---+-------------+--------------------+\n| 24|      Brok|  Male|  1|    Springate|vel accumsan tell...|\n| 99|     Trish|Female|  2|      Fielden|at feugiat non pr...|\n| 77|   Humfrey|  Male|  3|      Sparrow|mollis molestie l...|\n| 77|    Merrie|Female|  4|      Roddick|odio odio element...|\n| 51|  Isabella|Female|  5|  Myderscough|convallis tortor ...|\n| 30|    Blisse|Female|  6|        Caron|sit amet nulla qu...|\n|  8|  Gamaliel|  Male|  7|    Coulthart|at nibh in hac ha...|\n| 39|   Candide|Female|  8|   Tollemache|orci luctus et ul...|\n| 89|   Renaldo|  Male|  9|     Christin|eget elit sodales...|\n|  7| Stanfield|  Male| 10|       Pollak|eget eleifend luc...|\n| 93|    Leonid|  Male| 11|      Riddoch|nisi vulputate no...|\n| 76|     Chris|Female| 12|         Tape|vitae quam suspen...|\n| 90|    Sawyer|  Male| 13|      Bagnold|suscipit a feugia...|\n| 83|  Reinwald|  Male| 14|      Parslow|vestibulum alique...|\n| 71| Westbrook|  Male| 15|     Crewther|nunc proin at tur...|\n| 42|     Viole|Female| 16|      Blincko|curae mauris vive...|\n| 26|      Eddi|Female| 17|     Goligher|sit amet eros sus...|\n| 57|   Siobhan|Female| 18|     Leathart|tempus vivamus in...|\n| 24|  Northrop|  Male| 19|      Barkway|volutpat convalli...|\n|  9|   Myrtice|Female| 20|Grisenthwaite|hendrerit at vulp...|\n+---+----------+------+---+-------------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1601895541287_-1398209753","id":"20180226-215900_428859558","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41180"},{"text":"%md\n\n### User-Defined Function (UDF)\n\nLes UDF sont des fonctions définies par l’utilisateur à appliquer sur une table.\nIl est possible d’appliquer des udf pour chaque lignes ou de faire des aggrégation.","dateUpdated":"2020-10-05T10:59:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>User-Defined Function (UDF)</h3>\n<p>Les UDF sont des fonctions définies par l’utilisateur à appliquer sur une table.<br/>Il est possible d’appliquer des udf pour chaque lignes ou de faire des aggrégation.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1601895541287_-1398209753","id":"20180326-145726_1726479786","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41181"},{"text":"%md\n\n#### Première UDF\nNous souhaitons concatener les nom-prénoms en une seule colonne sous la forme \"NOM Prénom\" (le nom en majuscule).","dateUpdated":"2020-10-05T10:59:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Première UDF</h4>\n<p>Nous souhaitons concatener les nom-prénoms en une seule colonne sous la forme &ldquo;NOM Prénom&rdquo; (le nom en majuscule).</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1601895541288_-1400133498","id":"20180403-131052_555254237","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41182"},{"text":"%spark\n\n// Q6 : créez une fonction qui va concatener deux chaines de caractère, en mettant la première en majuscule.\ndef concatAndUpper(s1: String, s2 : String): String = {\n  s1.toUpperCase + \" \" + s2\n}\n\n// Q7 : Créez une udf à partir de la fonction crée. Une udf peut se déclarer de la façon suivante : udf((x:Int, y:Int) => maFonction(x, y))\nval concatAndUpperUDF = udf((x:String, y:String) => concatAndUpper(x, y))","dateUpdated":"2020-10-05T10:59:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"concatAndUpper: (s1: String, s2: String)String\nconcatAndUpperUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function2>,StringType,Some(List(StringType, StringType)))\n"}]},"apps":[],"jobName":"paragraph_1601895541288_-1400133498","id":"20180326-145759_2074143272","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41183"},{"text":"%spark\n\n// Q8 : Ajoutez une nouvelle colonne à notre Dataset utilisant l'udf crée, celle-ci va prendre deux paramètres de type Column.\n// Vous pouvez ensuite drop les colonnes \"first_name\" et \"last_name\"\nval df_name = ds.withColumn(\"name\", concatAndUpperUDF($\"last_name\", $\"first_name\")).drop(\"first_name\").drop(\"last_name\")\ndf_name.show","dateUpdated":"2020-10-05T10:59:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df_name: org.apache.spark.sql.DataFrame = [age: bigint, gender: string ... 3 more fields]\n+---+------+---+--------------------+--------------------+\n|age|gender| id|                text|                name|\n+---+------+---+--------------------+--------------------+\n| 24|  Male|  1|vel accumsan tell...|      SPRINGATE Brok|\n| 99|Female|  2|at feugiat non pr...|       FIELDEN Trish|\n| 77|  Male|  3|mollis molestie l...|     SPARROW Humfrey|\n| 77|Female|  4|odio odio element...|      RODDICK Merrie|\n| 51|Female|  5|convallis tortor ...|MYDERSCOUGH Isabella|\n| 30|Female|  6|sit amet nulla qu...|        CARON Blisse|\n|  8|  Male|  7|at nibh in hac ha...|  COULTHART Gamaliel|\n| 39|Female|  8|orci luctus et ul...|  TOLLEMACHE Candide|\n| 89|  Male|  9|eget elit sodales...|    CHRISTIN Renaldo|\n|  7|  Male| 10|eget eleifend luc...|    POLLAK Stanfield|\n| 93|  Male| 11|nisi vulputate no...|      RIDDOCH Leonid|\n| 76|Female| 12|vitae quam suspen...|          TAPE Chris|\n| 90|  Male| 13|suscipit a feugia...|      BAGNOLD Sawyer|\n| 83|  Male| 14|vestibulum alique...|    PARSLOW Reinwald|\n| 71|  Male| 15|nunc proin at tur...|  CREWTHER Westbrook|\n| 42|Female| 16|curae mauris vive...|       BLINCKO Viole|\n| 26|Female| 17|sit amet eros sus...|       GOLIGHER Eddi|\n| 57|Female| 18|tempus vivamus in...|    LEATHART Siobhan|\n| 24|  Male| 19|volutpat convalli...|    BARKWAY Northrop|\n|  9|Female| 20|hendrerit at vulp...|GRISENTHWAITE Myr...|\n+---+------+---+--------------------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1601895541288_-1400133498","id":"20180326-145844_1619300805","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41184"},{"text":"%md\n\nLes UDF sont également disponible lorsque l'on manipule une table temporaire en SQL\n","dateUpdated":"2020-10-05T10:59:01+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Les UDF sont également disponible lorsque l&rsquo;on manipule une table temporaire en SQL</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1601895541288_-1400133498","id":"20180403-154622_941507274","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41185"},{"text":"%spark\n\n// Nous enregistrons notre dataset en tant que table temporaire.\nds.createOrReplaceTempView(\"people\")\n\n// Q9 : Il est nécessaire de déclarer notre UDF grâce à la méthode spark.udf.register()\n// Documentation https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession@udf:org.apache.spark.sql.UDFRegistration\nspark.udf.register(\"concatAndUpperUDF\", concatAndUpperUDF)\n\n// Q9 bis : Utilisez votre udf pour obtenir un nouveau dataframe.\nspark.sql(\"select id, concatAndUpperUDF(last_name, first_name) as name, age, gender from people limit 6\").show\n","dateUpdated":"2020-10-05T10:59:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res379: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function2>,StringType,Some(List(StringType, StringType)))\n+---+--------------------+---+------+\n| id|                name|age|gender|\n+---+--------------------+---+------+\n|  1|      SPRINGATE Brok| 24|  Male|\n|  2|       FIELDEN Trish| 99|Female|\n|  3|     SPARROW Humfrey| 77|  Male|\n|  4|      RODDICK Merrie| 77|Female|\n|  5|MYDERSCOUGH Isabella| 51|Female|\n|  6|        CARON Blisse| 30|Female|\n+---+--------------------+---+------+\n\n"}]},"apps":[],"jobName":"paragraph_1601895541288_-1400133498","id":"20180403-133530_1689628386","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41186"},{"text":"%md\n\n### Quelques calculs sur notre Dataset / Dataframe\n\nAvec notre Dataset fortement typé nous pouvons le manipuler plus facilement qu'un dataframe.\nLe but de cette partie va être de calculer des indicateurs sur nos données.\nNous souhaitons connaitre l'âge moyen des adultes de notre population en fonction du sexe (Male / Female).","dateUpdated":"2020-10-05T10:59:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Quelques calculs sur notre Dataset / Dataframe</h3>\n<p>Avec notre Dataset fortement typé nous pouvons le manipuler plus facilement qu&rsquo;un dataframe.<br/>Le but de cette partie va être de calculer des indicateurs sur nos données.<br/>Nous souhaitons connaitre l&rsquo;âge moyen des adultes de notre population en fonction du sexe (Male / Female).</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1601895541288_-1400133498","id":"20180226-215913_680835949","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41187"},{"text":"%spark\n\nimport org.apache.spark.sql.expressions.scalalang.typed\n\n// Q10 : utilisez la méthode filter pour ne garder que les personnes âgées de plus de 18 ans.\nval dsAdulte = ds.filter(_.age > 18)\nval dsAdulte = ds.filter(person => person.age > 18)\n\n// Q11 : Calculez l'age moyen en fonction du sexe (Male / Female). \n// Commencez par grouper les entrées en fonction de la colonne \"gender\" puis faites une aggrégation pour calculer la moyenne. \n// Vous allez avoir besoin des fonctions groupByKey(), agg() et typed.avg()\nval ageMoyenAdulte = dsAdulte.groupByKey(_.gender).agg(typed.avg( _.age))\nval ageMoyenAdulte = dsAdulte.groupByKey(_.gender).agg(typed.avg[Person]( _.age).alias(\"average_age\").as[Double])\nageMoyenAdulte.show","dateUpdated":"2020-10-05T10:59:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.expressions.scalalang.typed\ndsAdulte: org.apache.spark.sql.Dataset[Person] = [age: bigint, first_name: string ... 4 more fields]\ndsAdulte: org.apache.spark.sql.Dataset[Person] = [age: bigint, first_name: string ... 4 more fields]\nageMoyenAdulte: org.apache.spark.sql.Dataset[(String, Double)] = [value: string, TypedAverage(Person): double]\nageMoyenAdulte: org.apache.spark.sql.Dataset[(String, Double)] = [value: string, average_age: double]\n+------+-----------------+\n| value|      average_age|\n+------+-----------------+\n|Female|59.03712871287129|\n|  Male|59.67990074441688|\n+------+-----------------+\n\n"}]},"apps":[],"jobName":"paragraph_1601895541288_-1400133498","id":"20180403-142929_1464052889","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41188"},{"text":"%md\n\n#### Seconde UDF\n\nNous allons cette fois utiliser une UDAF (User-Defined Aggregated Function)\nPour cela nous allons utiliser une classe Aggregator.\nDocumentation : https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Aggregator\n\nLe but va être d'arriver au même résultat qu'à la question Q11 en définissant à la main l'aggregation.","dateUpdated":"2020-10-05T10:59:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Seconde UDF</h4>\n<p>Nous allons cette fois utiliser une UDAF (User-Defined Aggregated Function)<br/>Pour cela nous allons utiliser une classe Aggregator.<br/>Documentation : <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Aggregator\">https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Aggregator</a></p>\n<p>Le but va être d&rsquo;arriver au même résultat qu&rsquo;à la question Q11 en définissant à la main l&rsquo;aggregation.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1601895541289_-1400518246","id":"20180326-152619_1848803917","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41189"},{"text":"%spark\n\nimport org.apache.spark.sql.expressions.Aggregator\nimport org.apache.spark.sql.Encoder\nimport org.apache.spark.sql.Encoders\n\n// La classe Average va nous permettre de stocker nos résultats intermédiaire (le buffer).\ncase class Average(var sum: Long, var count: Long)\n\n\n// Q12 : Construire un Aggregator\n\n// La classe Aggregator est abstraite, il y a des fonctions à implémenter.\nval avgAgeUDF = new Aggregator[Person, Average, Double] {\n    \n    // Fonction d'initialisation du buffer. Nous l'initialisons à sum=0, count=0.\n    def zero: Average = Average(0L, 0L)\n\n    // Le reduce permet de combiner la valeur du buffer avec une nouvelle valeur.\n    def reduce(buffer: Average, person: Person): Average = {\n      buffer.sum += person.age\n      buffer.count += 1\n      buffer\n    }\n    \n    // Merge de deux résultats intermédiaire.\n    def merge(b1: Average, b2: Average): Average = {\n      b1.sum += b2.sum\n      b1.count += b2.count\n      b1\n    }\n    \n    // Résultat final, après tous les reduce.\n    def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count\n    \n    // Encoder pour les valeurs intermédiaires\n    def bufferEncoder: Encoder[Average] = Encoders.product\n    // Encoder pour le résultat final\n    def outputEncoder: Encoder[Double] = Encoders.scalaDouble\n    \n}.toColumn.name(\"avg\")\n","dateUpdated":"2020-10-05T10:59:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.expressions.Aggregator\nimport org.apache.spark.sql.Encoder\nimport org.apache.spark.sql.Encoders\ndefined class Average\navgAgeUDF: org.apache.spark.sql.TypedColumn[Person,Double] = $anon$1() AS `avg`\n"}]},"apps":[],"jobName":"paragraph_1601895541289_-1400518246","id":"20180326-151132_813536505","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41190"},{"text":"%spark\n\n// Q13 : Nous pouvons maintenant utiliser notre UDAF. Il est toujours nécessaire de faire le groupByKey pour obtenir une aggrégation sur le sexe.\nval avgUDAF = dsAdulte.groupByKey(_.gender).agg(avgAgeUDF)\navgUDAF.show","dateUpdated":"2020-10-05T10:59:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"avgUDAF: org.apache.spark.sql.Dataset[(String, Double)] = [value: string, avg: double]\n+------+-----------------+\n| value|              avg|\n+------+-----------------+\n|Female|59.03712871287129|\n|  Male|59.67990074441688|\n+------+-----------------+\n\n"}]},"apps":[],"jobName":"paragraph_1601895541289_-1400518246","id":"20180326-151618_770657647","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41191"},{"text":"%spark\n","dateUpdated":"2020-10-05T10:59:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1601895541289_-1400518246","id":"20180326-151026_1632824497","dateCreated":"2020-10-05T10:59:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41192"}],"name":"2 - Exercices/Exercice 4 - Dataset","id":"2FNRTXCM4","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}