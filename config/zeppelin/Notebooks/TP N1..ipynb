{
  "metadata": {
    "name": "TP N1",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nSHOW DATABASES;\nUSE sales_db;\nSHOW TABLES;"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nSELECT * FROM sales_db.transactions LIMIT 10;"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n\ncat \u003e log4jLogs.txt\u003c\u003c EOF\nINFO 2025-09-21 10:00:00,001 Starting server\nERROR 2025-09-21 10:05:01,123 Failed connection\nWARN 2025-09-21 10:10:02,456 Slow response\nINFO 2025-09-21 10:15:03,789 User login successful\nEOF\n\n\ncat log4jLogs.txt\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "La commande %hdfs dans Zeppelin vous permet de créer et écrire des fichiers directement depuis un notebook."
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -put log4jLogs.txt /user/data/log4jLogs.txt\nhdfs dfs -chmod 777 /user/data/log4jLogs.txt\nhdfs dfs -ls /user/data/"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "CREATE [ EXTERNAL ] TABLE [ IF NOT EXISTS ] table_identifier\n    [ ( col_name1[:] col_type1 [ COMMENT col_comment1 ], ... ) ]\n    [ COMMENT table_comment ]\n    [ PARTITIONED BY ( col_name2[:] col_type2 [ COMMENT col_comment2 ], ... ) \n        | ( col_name1, col_name2, ... ) ]\n    [ CLUSTERED BY ( col_name1, col_name2, ...) \n        [ SORTED BY ( col_name1 [ ASC | DESC ], col_name2 [ ASC | DESC ], ... ) ] \n        INTO num_buckets BUCKETS ]\n    [ ROW FORMAT row_format ]\n    [ STORED AS file_format ]\n    [ LOCATION path ]\n    [ TBLPROPERTIES ( key1\u003dval1, key2\u003dval2, ... ) ]\n    [ AS select_statement ]\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nCREATE EXTERNAL TABLE IF NOT EXISTS log4jLogs (\n   t1 string, t2 string, t3 string, t4 string, \n   t5 string, t6 string, t7 string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY \u0027 \u0027\nSTORED AS TEXTFILE;\n\nLOAD DATA INPATH \u0027/user/data/log4jLogs.txt\u0027 INTO TABLE log4jLogs;"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nSHOW TABLES;\nSELECT * FROM default.log4jLogs LIMIT 10 ;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nDESCRIBE default.log4jlogs;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nDESCRIBE FORMATTED default.log4jlogs;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh \nhdfs dfs -chmod 777 hdfs://namenode:9000/user/hive/warehouse/\nhdfs dfs -chmod 777 hdfs://namenode:9000/user/hive/warehouse//log4jlogs\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\n-- Voir les détails complets de la table\nSHOW CREATE TABLE default.log4jlogs;"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\n-- Vérifier les propriétés de la table\nSHOW TBLPROPERTIES default.log4jlogs;"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Liste des tables de la database par défaaut : \"default\"\nspark.sql(\"SHOW TABLES\").show()\n\n# Liste de toutes les Databases présentes et de leurs tables :\nspark.sql(\"SHOW DATABASES\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Liste des tables dans toutes les databases :\ndatabases \u003d spark.sql(\"SHOW DATABASES\").collect()\nfor db in databases:\n    db_name \u003d db[0]\n    print(f\"Tables in database \u0027{db_name}\u0027:\")\n    spark.sql(f\"SHOW TABLES IN {db_name}\").show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Si la table existe mais n\u0027est pas visible : \n#### - Vous pourriez avoir besoin de rafraîchir le catalogue :\n%spark.pyspark\nspark.catalog.refreshTable(\"default.logjlogs\")\n#### - Vérifiez votre connexion au métastore Hive car on utilise des tables Hive\n#### - Vérifiez votre configuration Spark et vos connexions à la base de données\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Lecture directe à partir des fichiers \ndf \u003d spark.read.format(\"text\").load(\"hdfs://namenode:9000/user/hive/warehouse/log4jlogs\")\n\n# Ou, selon le format de la table :\n# df \u003d spark.read.format(\"parquet\").load(\"path/to/log4j/data/\")  # pour du parquet\n# df \u003d spark.read.format(\"delta\").load(\"path/to/log4j/data/\")    # pour des tables Delta\n# Iceberg ...\ndf.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql \nSHOW DATABASES;\nSHOW TABLES;"
    }
  ]
}